{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Chat Conversations Dataset</h2>\n",
    "<p>Dataset obtained exported from whatsapp, conversation spaning 18 months <br> </p>\n",
    "<p> Traditional Data Processing techniques applied to the dataset, such as: </p>\n",
    "\n",
    "<li> Lowercase all words </li>\n",
    "<li> Remove punctuation </li>\n",
    "<li> Remove Stop Words </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from future.utils import iteritems\n",
    "import string\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset </h3>\n",
    "<br>\n",
    "<i>Note:</i> <p> The dataset as exported OOB from Whatsapp is in .<i>txt</i> format, MS Excel was used to convert to csv using the <i> Get External data option </i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"data_conv.csv\"\n",
    "chat_df = pd.read_csv(chat, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data processing pipeline </h3>\n",
    "<br>\n",
    "<p> Functions defined to run on the dataset, mind a little bit the order of execution when calling these functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan(df):\n",
    "    \"\"\"Gets a DataFrame and drops all NaNs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        Dataframe containing the whatsapp exported conversation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        df without NaN in its rows\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def to_list(sentence):\n",
    "    \"\"\"Gets and converts to list the content of a dataframe cell\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        sentence of each line of whatsapp conversation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list with all its elements inside casted as string\n",
    "    \"\"\"\n",
    "    \n",
    "    return [str(sentence)]\n",
    "\n",
    "\n",
    "def filtering(df, column, value):\n",
    "    \"\"\"Gets a DataFrame and filter its contents based in specific value and for selected column \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        Dataframe containing the whatsapp exported conversation\n",
    "    \n",
    "    Column : str\n",
    "        Column name of the dataframe where the changes should take effect\n",
    "        \n",
    "    value : str\n",
    "        Value intended to be kept from specific column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        df filtered and displaying only the rows which contain the value passed to the function\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df[column].str.contains(value)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def remover(sentence):\n",
    "    \n",
    "    \"\"\"Gets the content of a cell in the dataframe, in this context a sentence and removes text \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : list\n",
    "        Sentence or text within the cell in the dataframe \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence: str\n",
    "        sentence without the hand-picked words chose to be removed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentence.remove('william')\n",
    "        sentence.remove('äóimage')\n",
    "        sentence.remove('äógif')\n",
    "        \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    finally:\n",
    "        for word in sentence:\n",
    "            if len(word)>=20 and word.startswith('http'):\n",
    "                sentence.remove(word)\n",
    "\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def lowercase(sentence):\n",
    "    \n",
    "    \"\"\"Gets the content of a cell in the dataframe, and lower case it \n",
    "     Returns a copy of the string in which all case-based characters have been lowercased\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        sentence within a df cell\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence: str\n",
    "        whole sentence in lowercase\n",
    "\n",
    "    \"\"\"   \n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    \"\"\"Gets the content of a cell in the dataframe, and tokenize it by white space \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : list\n",
    "        list[0] Sentence or text within the cell in the dataframe in a list \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens: list\n",
    "        list with word tokens, split by white spaces\n",
    "    \"\"\"\n",
    "    tokens = sentence[0].split(\" \")\n",
    "    return tokens\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    \"\"\"Gets the content of a cell in the dataframe, and remove punctuation signs\n",
    "    This function used translate(maketrans(from,to, del))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        Sentence or text within the cell in the dataframe in a list \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence: str\n",
    "        sentence without punctuation \n",
    "    \"\"\"\n",
    "    \n",
    "    punct = string.punctuation\n",
    "    return sentence.translate(sentence.maketrans(\"\",\"\", punct))\n",
    "\n",
    "\n",
    "def corpus_builder(df):\n",
    "    \n",
    "    \"\"\" Gets the whole dataset and makes one single corpus\n",
    "     out of every sentence.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Complete dataframe containing all the sentences. The dataframe\n",
    "        must contain a column for messages 'messages'\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    corpus: list\n",
    "        list containing all the words as tokens for the whole dataset\n",
    "        \n",
    "    \"\"\" \n",
    "    df.reset_index(inplace=True)\n",
    "    corpus = []\n",
    "    size = len(df)\n",
    "    for index in range(0, size):\n",
    "        corpus.extend(df['message'][index])\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "\n",
    "def word_ocurrences(corpora):\n",
    "    \"\"\" Gets the whole dataset made of tokens in a list and returns its frequency\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpora : list\n",
    "        All tokens that make the datasrt \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    max_result: tuple\n",
    "        Tuple containing the most written word (word, quantity)\n",
    "        \n",
    "    word_ocurrences: dict\n",
    "    \n",
    "        dictionary with Key = Word & Value = Number of times the word is in the\n",
    "        dataset.\n",
    "        \n",
    "    \"\"\"\n",
    "    words_ocurrences = defaultdict(int)\n",
    "    for word in corpora:\n",
    "        words_ocurrences[word] +=1\n",
    "    max_result = max(iteritems(words_ocurrences),key= lambda x: x[1])\n",
    "    return max_result, dict(words_ocurrences)\n",
    "\n",
    "\n",
    "def rank_words(word_count):\n",
    "    \"\"\" Gets the dictionary with the words and number of times this one is present unordered\n",
    "        and returns it ordered by value\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_count : dictionary\n",
    "        dictionary containing all the words and its number of times in which\n",
    "        they are present in the dataset WORD: #Times_in_dataset\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ranked_words: dictionaty\n",
    "        Dictionary with the word ocurrence sorted by  Desc - Value\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    ranked_words = {k:v for k, v in sorted(word_count.items(), key = lambda x: word_count[x[0]], reverse= True)}\n",
    "    return ranked_words\n",
    "\n",
    "def distinct_words(corpus):\n",
    "    unique_words =  list(set(corpus))\n",
    "    size_vocab = len(unique_words)\n",
    "    return unique_words, size_vocab\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data processing pipeline Applied </h3>\n",
    "<img src=\"image-pipe.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[30/07/2018, 10:54:40]</td>\n",
       "      <td>[so, its, william, here]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>äó_[30/07/2018, 11:03:</td>\n",
       "      <td>[53, omitted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[30/07/2018, 11:03:59]</td>\n",
       "      <td>[now, i, am, spamming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[30/07/2018, 12:50:55]</td>\n",
       "      <td>[i, came, to, have, lunch, with, the, guys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[30/07/2018, 12:50:59]</td>\n",
       "      <td>[and, i, am, dizzy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index               timestamp                                      message\n",
       "0      0  [30/07/2018, 10:54:40]                     [so, its, william, here]\n",
       "1      2  äó_[30/07/2018, 11:03:                                [53, omitted]\n",
       "2      3  [30/07/2018, 11:03:59]                       [now, i, am, spamming]\n",
       "3      5  [30/07/2018, 12:50:55]  [i, came, to, have, lunch, with, the, guys]\n",
       "4      6  [30/07/2018, 12:50:59]                          [and, i, am, dizzy]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NaN\n",
    "chat_df = drop_nan(chat_df)\n",
    "\n",
    "# Select only Messages from William\n",
    "\n",
    "chat_df = filtering(chat_df, 'message', 'William')\n",
    "\n",
    "# Lower Case the strings\n",
    "chat_df['message'] = chat_df['message'].apply(lowercase)\n",
    "\n",
    "\n",
    "# remove punctuaction from every sentence contained in the message column\n",
    "chat_df['message'] = chat_df['message'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "# Convert each cell of the DF in a list made of strings\n",
    "chat_df['message'] = chat_df['message'].apply(to_list)\n",
    "\n",
    "\n",
    "# Tokenize the list \n",
    "chat_df['message'] = chat_df['message'].apply(tokenizer)\n",
    "\n",
    "\n",
    "# I put it in the end, so every token is an element of the list, \n",
    "# before every sentence was a string, therefore remove failed\n",
    "chat_df['message'] = chat_df['message'].apply(remover)\n",
    "\n",
    "\n",
    "corpus = corpus_builder(chat_df)\n",
    "\n",
    "top, ocurrence = word_ocurrences(corpus)\n",
    "ranking = rank_words(ocurrence)\n",
    "\n",
    "\n",
    "chat_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Corpus </h3>\n",
    "<br>\n",
    "<p> This is the corpus I could use to analyze context, and predict words, is stored under <em>corpus</em> variable, on the other hand, the variable <em>ranking</em> has the corpus ordered by value ocurrence, losing all sequential meaning and more focus to be used in simple analysis </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'was', 'that', 'thing', 'that', 'sonja', 'said', 'about', 'chalenge', 'in', 'chatter',\n",
      " 'if', 'it', 'was', 'in', 'the', 'last', 'meeting', 'i', 'was', 'in', 'auto', 'pilot', 'and',\n",
      " 'cold', 'i', 'think', 'yeah', 'i', 'can', 'recall', 'that', 'saying', 'that', 'i', 'was', 'really',\n",
      " 'hungry', 'and', 'cold', 'i', 'think', 'i', 'just', 'remember', 'one', 'thing', 'and', 'it', 'is',\n",
      " 'you', 'got', 'a', 'diploma', 'and', 'there', 'was', 'a', 'white', 'dog', 'in', 'someone', 'wall',\n",
      " 'paper', 'hahahaha', 'she', 'said', 'that', 'wow', 'i', 'was', 'really', 'gone', 'sometimes',\n",
      " 'day', 'after', 'weed', 'i', 'am', 'still', 'quite', 'slow', 'hahahahaha', 'nice', 'way', 'to',\n",
      " 'see', 'it', 'i', 'like', 'dogs', 'and', 'cats', 'may', 'be', 'thats', 'why', 'it', 'was', 'the',\n",
      " 'coffee', 'talk', 'i', 'am', 'so', 'looking', 'forward', 'what', 'will', 'happen', 'i', 'want',\n",
      " 'to', 'see', 'it', 'will', 'give', 'me', 'a', 'new', 'story', 'to', 'tell', 'when', 'having', 'a',\n",
      " 'beer', '', 'so', 'i', 'was', 'in', 'this', 'company', 'that', 'got', 'acquired', 'hahahaha',\n",
      " 'yeah', 'thats', 'a', 'big', 'pain', 'in', 'the', 'ass', 'i', 'will', 'ask', 'steffen', 'tomorrow',\n",
      " 'if', 'it', 'makes', 'sense', 'to', 'do', 'the', 'roadmap', 'certification', 'because', 'now',\n",
      " 'the', 'question', 'is', 'is', 'there', 'any', 'roadmap', '', 'not', 'the', 'roadmap', 'of', 'the',\n",
      " 'product', 'but', 'the', 'company', 'i', 'read', 'that', 'document', 'and', 'i', 'can', 'only',\n",
      " 'recall', 'if', 'the', 'office', 'schedule', 'would', 'change', '', 'and', 'it', 'was', 'answered',\n",
      " 'like']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(corpus[200:400], compact=True, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset ordered by Value and saved as JSON </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sorted_vocabulary_cleaned.json', 'w') as sorted_vocab:\n",
    "    json.dump(ranking, sorted_vocab, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Unique words in dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_words, length = distinct_words(corpus)\n",
    "with open('unique_words_cleaned.csv', 'w', newline='') as unique_words:\n",
    "    wr = csv.writer(unique_words)\n",
    "    wr.writerow(dist_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Corpus with sequential order </h3>\n",
    "<br>\n",
    "<i>Note:</i> <p>Contains stop words</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dialogue.csv', 'w') as dialogue:\n",
    "    writer = csv.writer(dialogue)\n",
    "    writer.writerow(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
